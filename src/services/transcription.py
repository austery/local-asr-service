import asyncio
import logging
import os
import shutil
import tempfile
import time
from dataclasses import dataclass, field
from typing import Any

from fastapi import UploadFile
from starlette.concurrency import run_in_threadpool

from src.core.base_engine import ASREngine
from src.core.factory import create_engine_for_spec
from src.core.model_registry import ModelSpec


@dataclass
class TranscriptionJob:
    uid: str
    temp_dir: str
    temp_file_path: str
    params: dict[str, Any]
    future: asyncio.Future  # type: ignore[type-arg]
    received_at: float
    # None = use whatever model is currently loaded (no switch needed)
    requested_model_spec: ModelSpec | None = field(default=None)


class TranscriptionService:
    """
    ËΩ¨ÂΩïÊúçÂä°Ë∞ÉÂ∫¶Âô®„ÄÇ
    ËÅåË¥£Ôºö
    1. ÁÆ°ÁêÜÂºÇÊ≠•ÈòüÂàó (Async Queue)
    2. ÂçèË∞É Engine ËøõË°å‰∏≤Ë°åÊé®ÁêÜ
    3. Âú®‰∏§Ê¨°Êé®ÁêÜ‰πãÈó¥ÊåâÈúÄÁÉ≠Êç¢Ê®°Âûã
    4. ÁÆ°ÁêÜ‰∏¥Êó∂Êñá‰ª∂ÁöÑÁîüÂëΩÂë®Êúü
    """

    def __init__(
        self,
        engine: ASREngine,
        max_queue_size: int = 50,
        initial_model_spec: ModelSpec | None = None,
    ):
        self.engine = engine
        self._current_model_spec = initial_model_spec
        self._engine_degraded = False
        self.logger = logging.getLogger(__name__)
        self.queue: asyncio.Queue[TranscriptionJob | None] = asyncio.Queue(maxsize=max_queue_size)
        self.is_running = False
        self.logger.info(f"üö¶ Service initialized. Queue size: {max_queue_size}")

    @property
    def current_model_spec(self) -> ModelSpec | None:
        """The ModelSpec of the currently loaded engine (None if not tracked)."""
        return self._current_model_spec

    async def start_worker(self) -> None:
        """ÂêØÂä®ÂêéÂè∞Ê∂àË¥πËÄÖÂæ™ÁéØ (Âú® main.py ÁöÑ lifespan ‰∏≠Ë∞ÉÁî®)"""
        self.is_running = True
        asyncio.create_task(self._consume_loop())
        self.logger.info("üë∑ Background worker started.")

    async def submit(
        self,
        file: UploadFile,
        params: dict[str, Any],
        request_id: str = "unknown",
        model_spec: ModelSpec | None = None,
    ) -> str | dict[str, Any]:
        """
        Êèê‰∫§‰ªªÂä°Êé•Âè£ (‰æõ API Â±ÇË∞ÉÁî®)„ÄÇ
        model_spec=None Ë°®Á§∫‰ΩøÁî®ÂΩìÂâçÂ∑≤Âä†ËΩΩÁöÑÊ®°ÂûãÔºà‰∏çÊç¢Ê®°Ôºâ„ÄÇ
        """
        if self.queue.full():
            self.logger.warning(f"[{request_id}] Queue full, rejecting request")
            raise RuntimeError("Service busy: Queue is full.")

        temp_dir = tempfile.mkdtemp(prefix="asr_task_")

        try:
            file_ext = os.path.splitext(file.filename or "upload.wav")[1] or ".wav"
            temp_path = os.path.join(temp_dir, f"original{file_ext}")

            with open(temp_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)

            loop = asyncio.get_running_loop()
            future: asyncio.Future[str | dict[str, Any]] = loop.create_future()

            job = TranscriptionJob(
                uid=request_id,
                temp_dir=temp_dir,
                temp_file_path=temp_path,
                params=params,
                future=future,
                received_at=time.time(),
                requested_model_spec=model_spec,
            )

            await self.queue.put(job)
            result: str | dict[str, Any] = await future
            return result

        except Exception as e:
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir, ignore_errors=True)
            raise e

    async def stop_worker(self) -> None:
        """‰ºòÈõÖÂÅúÊ≠¢Ê∂àË¥πËÄÖÂæ™ÁéØ"""
        self.is_running = False
        await self.queue.put(None)  # type: ignore[arg-type]

    async def _switch_model(self, new_spec: ModelSpec, job_uid: str) -> None:
        """
        ÁÉ≠Êç¢Ê®°ÂûãÔºörelease ÊóßÂºïÊìé ‚Üí load Êñ∞ÂºïÊìé ‚Üí ÂéüÂ≠êÊõøÊç¢„ÄÇ

        Per SPEC-108 ¬ß5: release ÂèëÁîüÂú® load ‰πãÂâçÔºåÁ°Æ‰øù M-series ‰∏ä‰∏ç‰ºöÂèåÂÄçÂÜÖÂ≠òÂ≥∞ÂÄº„ÄÇ
        Â¶ÇÊûú load Â§±Ë¥•ÔºåÂ∞ùËØïÊÅ¢Â§çÂéüÂºïÊìéÔºõÊÅ¢Â§çÂ§±Ë¥•Êó∂ËÆ∞ÂΩïÈôçÁ∫ßÁä∂ÊÄÅÂπ∂ÁªßÁª≠„ÄÇ
        """
        old_engine = self.engine
        old_spec = self._current_model_spec
        old_alias = old_spec.alias if old_spec else "unknown"
        new_alias = new_spec.alias

        self.logger.info(f"[{job_uid}] üîÑ Switching model: {old_alias} ‚Üí {new_alias}")
        switch_start = time.time()

        # Step 1: Release old engine (blocking ‚Üí thread pool).
        # Per SPEC-108 ¬ß5: release MUST succeed before load to avoid double memory peak on M-series.
        # If release fails, abort the switch immediately ‚Äî do NOT proceed to load.
        try:
            await run_in_threadpool(old_engine.release)
        except Exception as e:
            self.logger.error(
                f"[{job_uid}] ‚ùå Old engine release failed: {e}. "
                f"Aborting switch to protect memory budget ‚Äî engine remains {old_alias}.",
                exc_info=True,
            )
            raise RuntimeError(
                f"Model switch aborted: failed to release '{old_alias}' ({e}). "
                f"The current engine is still loaded and usable."
            ) from e

        # Step 2: Create & load new engine (blocking ‚Üí thread pool)
        new_engine = create_engine_for_spec(new_spec)
        try:
            await run_in_threadpool(new_engine.load)
        except Exception as load_err:
            self.logger.error(
                f"[{job_uid}] ‚ùå New engine load failed: {load_err}. "
                f"Attempting to restore previous engine ({old_alias}).",
                exc_info=True,
            )
            try:
                await run_in_threadpool(old_engine.load)
                self.engine = old_engine
                self._current_model_spec = old_spec
                self.logger.info(f"[{job_uid}] ‚úÖ Restored previous engine: {old_alias}")
            except Exception as restore_err:
                # Both load and restore failed: engine is now in an undefined state.
                # Mark degraded so subsequent jobs fail fast with a clear message
                # instead of silently crashing against an unloaded engine.
                self._engine_degraded = True
                self.logger.error(
                    f"[{job_uid}] ‚ùå FATAL: Both model switch and recovery failed. "
                    f"load_err={load_err!r}, restore_err={restore_err!r}. "
                    f"Service is degraded ‚Äî manual restart required.",
                    exc_info=True,
                )
                raise RuntimeError(
                    f"Engine unrecoverable: switch to '{new_alias}' failed ({load_err}), "
                    f"restore of '{old_alias}' also failed ({restore_err}). "
                    f"Service must be restarted."
                ) from restore_err
            raise load_err  # Fail this job; engine restored, next job can proceed

        # Step 3: Atomic swap (single-threaded consumer ‚Äî no lock needed)
        self.engine = new_engine
        self._current_model_spec = new_spec

        elapsed = time.time() - switch_start
        self.logger.info(f"[{job_uid}] ‚úÖ Model switch complete: {new_alias} ({elapsed:.2f}s)")

    async def _consume_loop(self) -> None:
        """
        Ê∂àË¥πËÄÖÂæ™ÁéØ (Strict Serial Execution).
        This is the single thread of control for all inference ‚Äî no concurrency here.
        """
        while self.is_running:
            job: TranscriptionJob | None = await self.queue.get()

            if job is None:
                break

            queue_time = time.time() - job.received_at
            self.logger.info(f"[{job.uid}] Starting transcription (queue_time={queue_time:.2f}s)")

            inference_start = time.time()
            try:
                # === Degraded engine guard ===
                if self._engine_degraded:
                    raise RuntimeError(
                        "Service is in a degraded state (engine unrecoverable). "
                        "Manual restart required."
                    )

                # === Model switch check (before inference) ===
                requested = job.requested_model_spec
                if requested is not None and requested != self._current_model_spec:
                    await self._switch_model(requested, job.uid)

                # === Core inference ===
                output_format = job.params.get("output_format", "txt")
                with_timestamp = job.params.get("with_timestamp", False)

                result_data = await run_in_threadpool(
                    self.engine.transcribe_file,
                    file_path=job.temp_file_path,
                    language=job.params.get("language", "auto"),
                    output_format=output_format,
                    with_timestamp=with_timestamp,
                    use_itn=True,
                )

                inference_time = time.time() - inference_start
                total_time = time.time() - job.received_at

                if isinstance(result_data, dict):
                    result: str | dict[str, Any] = {
                        "text": result_data.get("text", ""),
                        "duration": total_time,
                        "segments": result_data.get("segments"),
                    }
                else:
                    result = result_data

                self.logger.info(
                    f"[{job.uid}] Transcription completed: "
                    f"format={output_format}, queue_time={queue_time:.2f}s, "
                    f"inference_time={inference_time:.2f}s"
                )

                if not job.future.done():
                    job.future.set_result(result)

            except Exception as e:
                self.logger.exception(f"‚ùå [{job.uid}] Job failed: {e}")
                if not job.future.done():
                    job.future.set_exception(e)

            finally:
                if os.path.exists(job.temp_dir):
                    shutil.rmtree(job.temp_dir, ignore_errors=True)
                self.queue.task_done()
