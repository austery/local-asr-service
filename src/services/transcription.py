import asyncio
import shutil
import os
import uuid
import time
import logging
import tempfile
from dataclasses import dataclass
from typing import Dict, Any
from fastapi import UploadFile
from starlette.concurrency import run_in_threadpool

# å¼•å…¥æŠ½è±¡æ¥å£
from src.core.base_engine import ASREngine

# å®šä¹‰ä¸€ä¸ªç®€å•çš„ä»»åŠ¡å¯¹è±¡ï¼Œç”¨äºåœ¨é˜Ÿåˆ—ä¸­ä¼ é€’
@dataclass
class TranscriptionJob:
    uid: str
    temp_dir: str  # ä»»åŠ¡ä¸“å±ä¸´æ—¶ç›®å½•
    temp_file_path: str # åŸå§‹æ–‡ä»¶è·¯å¾„
    params: Dict[str, Any]
    future: asyncio.Future
    received_at: float

class TranscriptionService:
    """
    è½¬å½•æœåŠ¡è°ƒåº¦å™¨ã€‚
    èŒè´£ï¼š
    1. ç®¡ç†å¼‚æ­¥é˜Ÿåˆ— (Async Queue)
    2. åè°ƒ Engine è¿›è¡Œä¸²è¡Œæ¨ç†
    3. ç®¡ç†ä¸´æ—¶æ–‡ä»¶çš„ç”Ÿå‘½å‘¨æœŸ
    """

    def __init__(self, engine: ASREngine, max_queue_size: int = 50):
        self.engine = engine
        self.logger = logging.getLogger(__name__)
        # æ ¸å¿ƒè®¾è®¡ï¼šä½¿ç”¨ asyncio.Queue å®ç°èƒŒå‹ (Backpressure)
        # å¦‚æœé˜Ÿåˆ—æ»¡ 50 ä¸ªï¼Œå‰ç«¯ä¼šç›´æ¥æ”¶åˆ° 503 é”™è¯¯ï¼Œä¿æŠ¤ç³»ç»Ÿä¸å´©æºƒ
        self.queue = asyncio.Queue(maxsize=max_queue_size)
        self.is_running = False
        self.logger.info(f"ğŸš¦ Service initialized. Queue size: {max_queue_size}")

    async def start_worker(self):
        """å¯åŠ¨åå°æ¶ˆè´¹è€…å¾ªç¯ (åœ¨ main.py çš„ lifespan ä¸­è°ƒç”¨)"""
        self.is_running = True
        asyncio.create_task(self._consume_loop())
        self.logger.info("ğŸ‘· Background worker started.")

    async def submit(self, file: UploadFile, params: Dict[str, Any], request_id: str = "unknown") -> Dict[str, Any]:
        """
        æäº¤ä»»åŠ¡æ¥å£ (ä¾› API å±‚è°ƒç”¨)ã€‚
        è¿™ä¸ªæ–¹æ³•æ˜¯éé˜»å¡çš„ï¼šå®ƒåªæ˜¯æŠŠä»»åŠ¡æ‰”è¿›é˜Ÿåˆ—ï¼Œç„¶åç­‰å¾…ç»“æœã€‚
        """
        # 1. æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦å·²æ»¡ (å¿«é€Ÿå¤±è´¥)
        if self.queue.full():
            self.logger.warning(f"[{request_id}] Queue full, rejecting request")
            raise RuntimeError("Service busy: Queue is full.")

        # 2. "ä¸´æ—¶æ–‡ä»¶ä¹‹èˆ" (The Temp File Dance)
        # ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„ä¸´æ—¶ç›®å½•ï¼Œæ–¹ä¾¿ç»Ÿä¸€æ¸…ç†
        temp_dir = tempfile.mkdtemp(prefix="asr_task_")
        
        try:
            file_ext = os.path.splitext(file.filename)[1] or ".wav"
            # æ–‡ä»¶åä½¿ç”¨ original ä»¥ä¾¿åŒºåˆ†ï¼Œä½†å®é™…ä¸Šåªè¦åœ¨ç›®å½•ä¸‹å°±è¡Œ
            temp_filename = f"original{file_ext}"
            temp_path = os.path.join(temp_dir, temp_filename)

            # å°†ä¸Šä¼ çš„æ–‡ä»¶æµå†™å…¥ç£ç›˜
            with open(temp_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)

            # 3. åˆ›å»ºä»»åŠ¡å¯¹è±¡
            loop = asyncio.get_running_loop()
            future = loop.create_future()
            
            job = TranscriptionJob(
                uid=request_id,  # ä½¿ç”¨ä¼ å…¥çš„ request_id
                temp_dir=temp_dir,
                temp_file_path=temp_path,
                params=params,
                future=future,
                received_at=time.time()
            )

            # 4. å…¥é˜Ÿ
            await self.queue.put(job)
            
            # 5. ç­‰å¾…å¤„ç†ç»“æœ (Await the future)
            # è¿™é‡Œçš„ await ä¼šæŒ‚èµ·å½“å‰è¯·æ±‚ï¼Œç›´åˆ°åå° worker å®Œæˆå¤„ç†
            result = await future
            return result

        except Exception as e:
            # å¦‚æœåœ¨å…¥é˜Ÿå‰å°±å¤±è´¥äº†ï¼Œç¡®ä¿æ¸…ç†ä¸´æ—¶ç›®å½•
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir, ignore_errors=True)
            raise e

    async def stop_worker(self):
        """ä¼˜é›…åœæ­¢æ¶ˆè´¹è€…å¾ªç¯"""
        self.is_running = False
        # æ”¾å…¥ None å“¨å…µå”¤é†’é˜»å¡åœ¨ queue.get() çš„æ¶ˆè´¹è€…
        await self.queue.put(None)  # type: ignore[arg-type]

    async def _consume_loop(self):
        """
        æ¶ˆè´¹è€…å¾ªç¯ (Strict Serial Execution)ã€‚
        è¿™æ˜¯ä¿æŠ¤ M4 Pro æ˜¾å­˜çš„å…³é”®ã€‚
        """
        while self.is_running:
            # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
            job: TranscriptionJob = await self.queue.get()

            # None å“¨å…µè¡¨ç¤ºè¯¥é€€å‡ºäº†
            if job is None:
                break
            
            queue_time = time.time() - job.received_at
            self.logger.info(f"[{job.uid}] Starting transcription (queue_time={queue_time:.2f}s)")
            
            inference_start = time.time()
            try:
                # === æ ¸å¿ƒæ¨ç†é€»è¾‘ ===
                # æå–è¾“å‡ºæ ¼å¼å‚æ•°
                output_format = job.params.get("output_format", "txt")
                with_timestamp = job.params.get("with_timestamp", False)
                
                # run_in_threadpool æ˜¯ä¸ºäº†æŠŠåŒæ­¥çš„ Engine ä»£ç æ”¾åˆ°çº¿ç¨‹æ± é‡Œè·‘
                # é˜²æ­¢é˜»å¡ asyncio çš„äº‹ä»¶å¾ªç¯
                result_data = await run_in_threadpool(
                    self.engine.transcribe_file,
                    file_path=job.temp_file_path,
                    language=job.params.get("language", "auto"),
                    output_format=output_format,
                    with_timestamp=with_timestamp,
                    use_itn=True
                )

                # è®¡ç®—æ¨ç†è€—æ—¶
                inference_time = time.time() - inference_start
                total_time = time.time() - job.received_at
                
                # å¤„ç†è¿”å›å€¼
                # Engine ç°åœ¨æ ¹æ® output_format è¿”å›ä¸åŒæ ¼å¼:
                # - txt/srt: è¿”å› str
                # - json: è¿”å› dict {"text": ..., "segments": [...]}
                if isinstance(result_data, dict):
                    # JSON æ ¼å¼ï¼Œæ·»åŠ  duration
                    result = {
                        "text": result_data.get("text", ""),
                        "duration": total_time,
                        "segments": result_data.get("segments")
                    }
                else:
                    # txt/srt æ ¼å¼ï¼Œç›´æ¥é€ä¼ å­—ç¬¦ä¸²
                    result = result_data
                
                # è®°å½•å®Œæˆæ—¥å¿—
                self.logger.info(
                    f"[{job.uid}] Transcription completed: "
                    f"format={output_format}, queue_time={queue_time:.2f}s, inference_time={inference_time:.2f}s"
                )
                
                # å”¤é†’ç­‰å¾…çš„ API è¯·æ±‚
                if not job.future.done():
                    job.future.set_result(result)


            except Exception as e:
                self.logger.exception(f"âŒ [{job.uid}] Job failed: {e}")
                if not job.future.done():
                    job.future.set_exception(e)
            
            finally:
                # === æ‰“æ‰«æˆ˜åœº ===
                # æ— è®ºæˆåŠŸå¤±è´¥ï¼Œå¿…é¡»åˆ é™¤ä¸´æ—¶ç›®å½•
                # è¿™ä¼šè¿å¸¦åˆ é™¤åŸå§‹æ–‡ä»¶ã€å½’ä¸€åŒ–æ–‡ä»¶ã€åˆ‡ç‰‡æ–‡ä»¶ç­‰æ‰€æœ‰ä¸­é—´äº§ç‰©
                if os.path.exists(job.temp_dir):
                    shutil.rmtree(job.temp_dir, ignore_errors=True)
                
                # æ ‡è®°é˜Ÿåˆ—ä»»åŠ¡å®Œæˆ
                self.queue.task_done()